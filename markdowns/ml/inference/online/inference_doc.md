# Online inference

Deploy model for real-time inference via API or offline batch predictions.